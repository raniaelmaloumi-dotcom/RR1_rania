---
title: "(Homework for Week 6) Second draft: General Linear model"
format: 
  html:
    self_contained: true
    toc: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false 
  error: false
editor: visual
---

## Introduction

This paper performs a statistical analysis on openly available, open-source data collected through the Which Personality Quiz, an online personality-matching survey that associates participants with fictional characters from selected universes such as TV shows, movies, or popular franchises.

#### The survey:

The survey unfolds in three main stages.

**Participant personality:** First, participants answer questions about their personality using bipolar adjective scales ranging from 0 to 100. For example: position yourself between “introverted”(0%) and “extroverted” (100%.

**Universe selection and matching:** Then, participants are given a list of fictional universes (shows and movies) to select from. Based on these responses (embodied in the quiz_items variable and universe_selected), an algorithm matches participants with a character from the fictional universes selected and assigns a percentage indicating the degree of similarity.

**Rating section:** Participants may then evaluate the outcome of the matching by rating how similar they feel to the assigned character on a scale from 1 (“extremely similar”) to 6 (“extremely different”). This subjective evaluation (the variable survey_ratings) constitutes the main outcome variable in this study.

The analysis focuses on a subset of variables capturing the context of participation (year, region and country, occupation), personality-related information (Jung personality type and quiz item responses), participants’ evaluations of the character matching, and several time-lapse measures (in milliseconds) recording the time spent at different stages of the survey.

## Research Question:

In this report, we want to put to the test whether these similarity ratings are linked to participants' actual feelings of similarity to their result characters, or whether they are due to external factors, specifically cultural relevance.

Highly popular fictional universes tend to be more culturally salient, widely referenced, and familiar to participants. This greater exposure may foster positive associations and stronger memory traces. As a result, being matched with a character from a popular universe may be perceived as more similar, independently of the objective personality match.

In contrast, less popular or more niche universes, while they can still elicit nostalgia or emotional attachment for some participants, are generally less culturally present and may therefore generate weaker identificatory responses on average.

Another important factor in cultural relevance is the time in which the survey was taken. Established popular shows might overtime be surpassed by newly airing, less popular shows that are gradually discovered by participants. This could lead to higher identification with characters of these shows which participants most likely engage with more faithfully.

**Hypothesis1:** It is likely that higher universe popularity, captured by the number of selections by participants, is associated with lower similarity ratings (higher perceived similarity). Participants matched with characters from more popular fictional universes are therefore expected, on average, to rate the character match as more similar than those matched with characters from less popular universes.

**Hypothesis 2:** As survey years progress, similarity ratings for characters from less popular universes are expected to decrease (for increased perceived similarity), while ratings for popular universes remain relatively stable.

## Tidying the data:

The descriptive analysis centers on the "survey_ratings" variable, which captures participants’ evaluations of their character matches. This variable requires substantial preprocessing, as it is stored as a complex nested JSON structure showing ratings across different selected universes and matched characters.

Once transformed into a tidy format, this variable allows for the identification of fictional universes, the measurement of their popularity based on frequency of selection, and the comparison of evaluation scores between more and less popular universes. These descriptive statistics provide the empirical basis for assessing whether perceived similarity systematically varies with universe popularity.

```{r}
#| include: false
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(gt)
library(jsonlite)
library(purrr)
library(kableExtra)
library(knitr)
data <- read_delim("SWCPQ-Identification-Survey-Dataset-July2022.csv")

```

The dataset contains 297,877 observations and 19 variables. Several variables are stored as numeric (`<dbl>`) despite representing categorical information (age group , gender and screen type). These variables will be recoded as factors. Other variables contain JSON-encoded structures (e.g. survey ratings, quiz items) and require further parsing and tidying before analysis.

```{r}
#Constructing a table (called variable_table) to calculate missing values and number of unique values 
variable_table <- data %>%
  summarise(across(
    everything(), #selecting and summarizing onfo on all variables
    list(
      type = ~ class(.x)[1],
      n_missing = ~ sum(is.na(.x)),
      n_unique = ~ n_distinct(.x)
    ),
    .names = "{.col}__{.fn}"
  )) %>%
  pivot_longer(
    cols = everything(),
    names_to = c("variable", ".value"),
    names_sep = "__"
  ) %>%
  arrange(variable)

variable_table %>% #creating a table summarizing this information
  kable(
    col.names = c("Variable", "Type", "Missing values", "Unique values"),
    caption = "Overview of variables in the dataset"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )

```

Variables with substantial missing values (occupation, Jung type, country and region) will be removed from the main analysis to avoid faulty or unreliable conclusions.

```{r}
#Let's create a new dataset (called data_clean), showing only relavant information for analysis. 

#1. First removing non-reliable variables: 
data_clean <- data %>%

  select(
    -occupation,
    -jung_type,
    -country,
    -universes_unselected,
    -introelapse,
    -endelapse,
    -region
  ) %>%
  
# 2. Recoding coded numerics as factors 
  mutate(
    age_group = factor(age_group),
    gender    = factor(gender),
    engNat    = factor(engNat),
    screen    = factor(screen),
    
#3 Encoding quantitative variables are numeric
    year          = as.numeric(year),
    testelapse    = as.numeric(testelapse),
    surveyelapse  = as.numeric(surveyelapse),
    form          = as.numeric(form),
    enneagram_type= as.numeric(enneagram_type)
  )

```

Our new dataset contains 12 variables instead of 19.

Here we will focus on nested variables like survey_ratings and quiz_items in order to simplify analysis. The variable survey_ratings has 3 pieces of information (volumes) in one cell:

1.  *character ID:* the character participants were matched with after responding

2.  *user rating*: Participants' rating of character matching which goes from 1= Extremely similar to 6= Extremely different. This can be perceived as participants' satisfaction with survey results.

3.  *response time*: in milliseconds, how long did users take to rate the survey.

    ```{r}
    #Creating a rating block in the data_clean set. 
    #First we need to add user id s in order to locate which participants gave which rating: 
    data_clean <- data_clean %>%
      mutate(user_id = row_number()) %>% #user_id is given by row number
      relocate(user_id)

    #There are too many observations, which slows down the tidying process: we selected a random sample of 10 000 from the original dataset, data_clean
    set.seed(123)
    data_sample <- data_clean %>% 
      slice_sample(n= 10000)


    #Lets now seperate the three objects nested withing json_parsed (survey_rating list) into three usable variables

    data_sample_tidy <- data_sample %>%
      mutate(parsed = map(survey_ratings, fromJSON)) %>% #seperating volumes
      mutate(parsed = map(parsed, as.data.frame)) %>% #each volum as a column
      unnest(parsed) %>%
      rename(
        chr_id = V1,           # renaming each volume of the parsed list as their variable names.
        user_rating = V2,
        response_time = V3
      ) %>%
      mutate(
        user_rating = as.numeric(user_rating),   #user_rating is a numeric variable
        response_time = as.numeric(response_time) #same for response_time
      ) %>%
      select(-survey_ratings)

    #Now the section survey_ratings is transformed into three distinct variables each analysable independently. 
    summary(data_sample_tidy$user_rating) #measures of central tendency 
    ```

Having organised the major part of the data set of interest, we can now proceed with the visualization of distribution of key variables.

## Descriptive statistics:

### Hypothesis 1: Popular shows get better similarity ratings

##### Which universes do participants select the most ?

The variable universe_selected represents the list of fictional universes participants chose. Characters of these selected universes alone are matched with participants.

```{r}
#Universe_selected has multiple universes per row, lets split them:
universe_long <- data_sample_tidy %>%
  select(user_id, universes_selected) %>% 
  separate_rows(universes_selected, sep = ",") #seperating each universe selected by a participant to become an individual row rather than seperated by ","

#Now counting how many participants per universe
universe_counts <- universe_long %>%
  distinct(user_id, universes_selected) %>%  #making sure each participant's selection is counted 
  count(universes_selected, name = "n_participants") %>%
  arrange(desc(n_participants)) #decreasing order 
```

Lets first see the distribution of universes across participants:

```{r}
#Let's plot this new data frame to show the distribution of shows across participants.
ggplot(universe_counts,
       aes(x = reorder(universes_selected, n_participants), #starting from least to most picked)
           y = n_participants)) +
  geom_col() +
  coord_flip() +
  labs(
    x = "Fictional universe",
    y = "Number of participants",
    title = "Distribution of universe selection across participants"
  ) +
  theme_minimal()

```

The distribution of universes across participants is highly skewed. Most universes are selected by a relatively small number of participants. There is a high concentration of participants around a handful of very popular shows.

To operationnalize popularity as a category, we will introduce a median split separating higher than median selection as "popular" and lower than median as "unpopular".

Lets introduce a median split:

```{r}
#Introducing a median split of number of selection across universes, which we'll call median_selection.
median_selection <- median(universe_counts$n_participants)
median_selection
```

Our analysis categorizes universes by popularity, which here we compute by *frequency of selection*. We calculate the median of selection , which amounts to `{r} median_selection`\`. The median tells us that half of the available fictional universes were selected by fewer than 49 participants, and the other half is selected by more than 49 participants. This is the criteria we set to differentiate between popular and unpopular universes.

This median split allows us to both include all universes in our analysis and compare the mean ratings among two groups of different popularity levels. Lets classify the universes based on this criteria:

```{r}
#Building distinct groups by introducing the median split as a criteria. 
universe_counts <- universe_counts %>%
  mutate(
    popularity_group = if_else( #condition of median
      n_participants > median_selection,
      "Popular",
      "Unpopular" 
    )
  ) 
```

##### How do ratings differ by popularity ?

Now we want to see how each group performs on rating. Meaning, how similar do participants feel to the character they were matched with from a popular versus unpopular universe.

Reminder: The variable user rating is a Likert scale from 1 to 6, 1 being extremely similar and 6 extremely different. So here, the higher the rating, the less satisfied participants are with their matching.

```{r}
#dataframe that associates each universe with its rating 
ratings_by_universe <- data_sample_tidy %>% #extracting from original dataset
  select(user_id, chr_id, user_rating) %>%
  separate(chr_id, into = c("universe", "character_id"), sep = "/", remove = FALSE) %>%
  mutate(user_rating = as.numeric(user_rating)) #this allows us to work with means

#Now we show ratings by popularity of universes (bringing the criteria in the dataframe): 
ratings_with_popularity <- ratings_by_universe %>% 
  left_join(
    universe_counts %>% 
      select(universes_selected, popularity_group), #each universe is given its criteria depending on its rating
    by = c("universe" = "universes_selected")
  )

#Now calculating the mean, median and standard deviation of ratings by popularity
ratings_with_popularity %>%
  group_by(popularity_group) %>%
  summarise(
    n_ratings    = n(),
    mean_rating = mean(user_rating, na.rm = TRUE),
    median_rating = median(user_rating, na.rm = TRUE),
    sd_rating   = sd(user_rating, na.rm = TRUE)
  ) %>%
  mutate(
    mean_rating = round(mean_rating, 2),
    sd_rating   = round(sd_rating, 2)
  ) %>%
  kable(
    col.names = c(
      "Universe popularity",
      "Number of ratings",
      "Mean rating",
      "Median rating",
      "Standard deviation"
    ),
    caption = "Descriptive statistics of user ratings by universe popularity"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )
```

The mean ratings of popular versus unpopular shows is almost identical, with a difference of 0.06. Popular and unpopular universe groups seem to be associated with, on average, highly similar ratings. Lets visualize this with a box plot.

```{r}
#visualization of ratings by popularity: 

ggplot(
  ratings_with_popularity,
  aes(x = popularity_group, y = user_rating)
) +
  geom_boxplot() +
  labs(
    title = "Distribution of character ratings by universe popularity",
    x = "Universe popularity (median split)",
    y = "User rating (1 = extremely similar, 6 = extremely different)"
  ) +
  theme_minimal()

```

Indeed, as shown in this plot, popularity does not seem to be significantly associated with a change in rating. Although the range of ratings is higher in popular groups (which is expected), the means and standard deviations of the popularity categories are identical. Let's run a t-test to see if this is statistically sound.

##### Statistical test for this association (or lack thereof)

```{r}

t.test(user_rating ~ popularity_group,
       data = ratings_with_popularity)


```

We performed a simple t-test given we are comparing two categorical variables. Popularity_group is a binary variable with two categories: popular and unpopular. And rating is a ordinal categorical variable with six categories ranging from 1= Extremely similar to 6= extremely different.

The results of this test point to the rejection of our hypothesis. Although there is a difference between popular and unpopular universes, this difference is extremely small. The confidence interval is between a positive and negative value, making it likely that the difference between our two groups is null. The effect size (t= 1.46) suggests that this difference is negligeable. The p-value also suggests low statistical significance, meaning that this difference could be due to random variation in the sample.

### Hypothesis 2: Year affects popular shows similarity negatively

##### How many participants took the survey throughout the years:

```{r}
#Renaming the variable year as survey_year: 
data_sample_tidy <- data_sample_tidy %>%
  rename (survey_year = year)


#Barchart to visualise distribution of participants across years. 

data_sample_tidy %>%
  count(survey_year) %>%
  ggplot(aes(x = factor(survey_year), y = n)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Number of participants by survey year",
    x = "Survey year",
    y = "Number of participants"
  ) +
  theme_minimal()


```

Survey taking years range from 2020 to 2022, meaning three years. The number of participants decreased as time went by, by 50 000. 2020 had almost 80 000 participants, compared to 50 000 in 2021 and 30 000 in 2022.

##### Progression of ratings for all universes throughout these three years

```{r}
#First computing the average user_rating by year: 
avg_rating_year <- data_sample_tidy %>%
  group_by(survey_year) %>%
  summarise(
    mean_rating = mean(user_rating, na.rm = TRUE),
    n = n()
  )
#Now plotting this progression: 
ggplot(avg_rating_year, aes(x = factor(survey_year), y = mean_rating)) +
  geom_point(size = 4) + #for clarity, we opt for points 
  geom_text(
    aes(label = round(mean_rating, 3)),
    vjust = -1,
    size = 4
  ) +
  labs(
    title = "Average similarity rating by survey year",
    x = "Survey year",
    y = "Average similarity rating"
  ) +
  ylim(3.85, 4.00) +
  theme_minimal()

```

All average ratings for the three years of survey taking are around 4 (4="slightly different"). The average ratings from 2020 to 2021 went down 0.053, and then went back up by 0.034 in 2022. This indicates that overall, participants in 2021 were slightly more satisfied with their character matching compared to the previous and next year. Lets now see this progression factoring in popularity of universes.

##### Change in rating over the years, by popularity:

```{r}
#new dataframe, showing each participant and year of taking survey
user_year <- data_sample_tidy %>%
  select(user_id, survey_year) %>%
  distinct(user_id, .keep_all = TRUE)

#Now merging dataframe created earlier showing universes and rating and their category  
ratings_with_popularity <- ratings_with_popularity %>%
  left_join(user_year, by = "user_id")


#Now we can see for each year, we see mean rating of all popular vs unpopular universes 
avg_rating_year_pop <- ratings_with_popularity %>%
  group_by(survey_year, popularity_group) %>%
  summarise(
    mean_rating = mean(user_rating, na.rm = TRUE),
    n_participant = n(),
    .groups = "drop"
  )
#putting this info in a table 
avg_rating_year_pop %>%
  mutate(
    mean_rating = round(mean_rating, 2)
  ) %>%
  kable(
    col.names = c(
      "Survey year",
      "Universe popularity",
      "Mean rating",
      "Number of participants"
    ),
    caption = "Mean similarity ratings by universe popularity and survey year"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )



```

The ratings across both years and popularity do not fluctuate a lot.

```{r}
#Visualising these fluctuations 
ggplot(
  avg_rating_year_pop,
  aes(
    x = factor(survey_year),
    y = mean_rating,
    color = popularity_group,
    group = popularity_group
  )
) +
  geom_point(size = 3) +
  geom_line() +
  geom_text(
    aes(label = round(mean_rating, 3)),
    vjust = -1,
    size = 3,
    show.legend = FALSE
  ) +
  labs(
    title = "Average similarity rating by survey year, by universe popularity",
    x = "Survey year",
    y = "Average similarity rating",
    color = "Universe popularity"
  ) +
  theme_minimal()
```

Given that lower similarity ratings correspond to greater perceived similarity, the results suggest a divergence in similarity ratings across survey years by universe popularity. While similarity ratings for characters from popular universes remain relatively stable over time, ratings for characters from less popular universes decrease, indicating increasing perceived similarity. This pattern suggests that less popular universes may gain identificatory potential over time as they become more familiar to participants.

To understand more thoroughly the potential relationship between these three variables (ratings, year and popularity), and to also make sure this increase in similarity perception in unpopular universes is not due to less participants taking the survey through 2021 and 2022, we'll make a general linear model.

#### General Linear Model

In this model, our outcome variable (X) is the rating given by participants (numeric, roughly continuous). Our predictors are popularity, which is a categorical variable (pop vs unpop universe) and survey_year (numeric and continuous).

This model would help us describe the rating a participant would give depending on the year in which they are taking the survey, and whether the character they are rating is in a popular or unpopular show.

```{r}
#First we will need to center the year variable so the model doesnt give us counterintuitive intercept and estimates.
ratings_with_popularity <- ratings_with_popularity %>%
  mutate(survey_year_c = survey_year - 2020)

#Linear model modeling rating by popularity and year.
model_linear_year_c <- glm(
  user_rating ~ popularity_group * survey_year_c,
  data = ratings_with_popularity,
  family = gaussian()
)

summary(model_linear_year_c)
```

The regression analysis shows no statistically significant baseline difference in similarity ratings between popular and unpopular universes in the reference year 2020 (p = 0.74). Likewise, there is no evidence of a general time trend in ratings for popular universes, as the main effect of survey year is effectively zero and not statistically significant (p = 0.997). In contrast, the interaction between survey year and universe popularity is statistically significant (p \< 0.001), indicating that similarity ratings for unpopular universes decrease slightly more over time than those for popular universes. Although the magnitude of this interaction effect is small, it suggests a modest divergence in rating trajectories: perceived similarity with characters from unpopular universes increases over time relative to popular universes, even in the absence of overall time effects or baseline popularity differences.

```{r}
summary(lm(user_rating ~ popularity_group * survey_year_c,
           data = ratings_with_popularity))$r.squared
```

```{r}
anova(model_linear_year_c, type=2)
```
