---
title: "Final Report: Which Personality Quiz, Analysis of popularity throughout the years"
format: 
  html:
    self_contained: true
    toc: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false 
  error: false
editor: visual
---

## Introduction

This paper performs a statistical analysis on open-source data collected through the Which Personality Quiz, an online personality-matching survey that associates participants with fictional characters from TV shows, movies, or popular franchises.

### The survey:

The survey unfolds in three main stages.

-   **Participant personality:** First, participants answer questions about their personality using bipolar adjective scales ranging from 0 to 100. Participants slide the lever and stop at the percentage they feel fits their personality most. An example question is shown here:

![](images/clipboard-947655878.png)

The answers to these questions are encrypted into the variable quiz_items. There are 36 questions per participant.

-   **Universe selection and matching:** Then, participants are given a list of fictional universes (shows and movies) to select from.

![](images/clipboard-3461495799.png){width="484"}

Participants' selection of universes is encrypted into the variable universe_selected. Based on these responses (embodied in the quiz_items variable and universe_selected), an algorithm matches participants with a character from the fictional universes selected and assigns a percentage indicating a degree of similarity.

-   **Rating section:** Participants may then evaluate the outcome of the matching by rating how similar they feel to the assigned character on a scale from 1 (‚Äúextremely similar‚Äù) to 6 (‚Äúextremely different‚Äù). This subjective evaluation (the variable survey_ratings) constitutes the main outcome variable in this study.

There are many other variables of interest in this data set, however for this report, we choose to focus on three variables: universe_selected, survey_ratings and year.

## Research Question:

This Quiz is inspired by and highly resembles Buzzfeed quizzes. To gain traction and elicit positive responses from users, these quizzes rely heavily on popular franchises that ignite nostalgic feelings, like Harry Potter, the Marvel Universe and other widely known shows.

In this report, we want to put to the test whether users objectively feel similar to characters from the shows they selected, or whether they are led to appreciate a similarity rating because of the popularity of a show.

Highly popular fictional universes tend to be more culturally salient, widely referenced, and familiar to participants. This greater exposure may foster positive associations and stronger memory traces. As a result, it is possible that participants assign a better rating score just because they *want* to be similar to a character from a [famous]{.underline} show.

In contrast, less popular or more niche universes, while they can still elicit emotional attachment for some participants, are generally less culturally present and may therefore generate weaker ratings.

**How does this translate in our data?**

As we've mentioned before, our three variables of interest are *universe_selected* (which shows participants selected most and least), *survey_ratings* (how similar participants feel to their result character) and *year* (the year in which participants took the test, this variable will become relevant a little later into the analysis).

**Hypothesis 1:**

We predict that on average, universes that are "popular" (selected more often than others) will benefit from better similarity ratings (ratings closer to 1 for maximum similarity, as opposed to 6).

## Tidying the data:

```{r}
#| include: false
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(gt)
library(jsonlite)
library(purrr)
library(kableExtra)
library(knitr)
library(car)
library(broom)
library(modelsummary)
data <- read_delim("SWCPQ-Identification-Survey-Dataset-July2022.csv")

```

The dataset contains 297,877 observations and 19 variables. Several variables are stored as numeric (`<dbl>`) despite being categorical (age group , gender and screen type). These variables will be recoded as factors. Other variables contain JSON-encoded structures (survey_ratings and quiz_items) and require further parsing and tidying before analysis.

```{r}
#Constructing a table (called variable_table) to calculate missing values and number of unique values 
variable_table <- data %>%
  summarise(across(
    everything(), #selecting and summarizing info on all variables
    list(
      type = ~ class(.x)[1],
      n_missing = ~ sum(is.na(.x)), #show n of missing values
      n_unique = ~ n_distinct(.x)  #show n of unique values
    ),
    .names = "{.col}__{.fn}"
  )) %>%
  pivot_longer(
    cols = everything(),
    names_to = c("variable", ".value"),
    names_sep = "__"
  ) %>%
  arrange(variable)

variable_table %>% #creating a table summarizing this information
  kable(
    col.names = c("Variable", "Type", "Missing values", "Unique values"),
    caption = "Overview of variables in the dataset"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )

```

Variables with substantial missing values (occupation, Jung type, country and region) will be removed from the main analysis to avoid faulty or unreliable conclusions. These variables aren't relevant to our analysis anyway, and thus no longer need to appear.

```{r}
#Let's create a new dataset (called data_clean), showing only relavant information for analysis. 

#1. First removing non-reliable variables: 
data_clean <- data %>%

  select(
    -occupation,
    -jung_type,
    -country,
    -universes_unselected,
    -introelapse,
    -endelapse,
    -region
  ) %>%
  
# 2. Recoding coded numerics as factors 
  mutate(
    age_group = factor(age_group),
    gender    = factor(gender),
    engNat    = factor(engNat),
    screen    = factor(screen),
    
#3 Encoding quantitative variables are numeric
    year          = as.numeric(year),
    testelapse    = as.numeric(testelapse),
    surveyelapse  = as.numeric(surveyelapse),
    form          = as.numeric(form),
    enneagram_type= as.numeric(enneagram_type)
  )

```

Our new dataset contains 13 variables instead of 19.

Here we will focus on the nested variable *survey_ratings* in order to simplify analysis. The variable survey_ratings has 3 pieces of information (volumes) in one cell:

1.  *character ID:* the character participants were matched with after responding

2.  *user_rating*: Participants' rating of character matching which goes from 1= Extremely similar to 6= Extremely different. This can be perceived as participants' satisfaction with survey results.

3.  *response time*: in milliseconds, how long did users take to rate the survey.

    ```{r}
    #Creating a rating block in the data_clean set. 
    #First we need to add user id s in order to locate which participants gave which rating: 
    data_clean <- data_clean %>%
      mutate(user_id = row_number()) %>% #user_id is given by row number
      relocate(user_id)

    #There are too many observations, which slows down the tidying process: we selected a random sample of 10 000 from the original dataset, data_clean

    set.seed(123) #this freezes the sample to make sure results dont change everytime code is ran

    data_sample <- data_clean %>% 
      slice_sample(n= 10000)


    #Lets now seperate the three objects nested withing json_parsed (survey_rating list) into three usable variables

    data_sample_tidy <- data_sample %>%
      mutate(parsed = map(survey_ratings, fromJSON)) %>% #seperating volumes
      mutate(parsed = map(parsed, as.data.frame)) %>% #each volum as a column
      unnest(parsed) %>%
      rename(
        chr_id = V1,           # renaming each volume of the parsed list as their variable names.
        user_rating = V2,
        response_time = V3
      ) %>%
      mutate(
        user_rating = as.numeric(user_rating),   #user_rating is a numeric variable
        response_time = as.numeric(response_time) #same for response_time
      ) %>%
      select(-survey_ratings)

    #Now the section survey_ratings is transformed into three distinct variables each analysable independently. 

    # Measures of central tendency 
    kable(t(summary(data_sample_tidy$user_rating)),
          caption = "Measures of central tendency: survey ratings") %>%
      kable_styling(full_width = FALSE)
    ```

Mean rating is ‚âÉ 4 meaning that on average, participants found characters to be slightly different from themselves.

Having organised the major part of the data set of interest, we can now proceed with the visualization of distribution of key variables.

## Descriptive statistics:

### Hypothesis 1: Popular shows get better similarity ratings

##### Which universes do participants select the most ?

The variable *universe_selected* represents the list of fictional universes participants chose. Characters of these selected universes alone are matched with participants. This variable is also comprised of lists of universes per participant. To simplify visualization, we'll separate these lists into individual rows.

```{r}
#Universe_selected has multiple universes per row, lets split them:
universe_long <- data_sample_tidy %>%
  select(user_id, universes_selected) %>% 
  separate_rows(universes_selected, sep = ",") #seperating each universe selected by a participant to become an individual row rather than seperated by ","

#Now counting how many participants per universe
universe_counts <- universe_long %>%
  distinct(user_id, universes_selected) %>%  #making sure each participant's selection is counted 
  count(universes_selected, name = "n_participants") %>%
  arrange(desc(n_participants)) #decreasing order 
```

Lets first see the distribution of universes across participants:

```{r}
#Let's plot this new data frame to show the distribution of shows across participants.
ggplot(universe_counts,
       aes(x = reorder(universes_selected, n_participants), #starting from least to most picked)
           y = n_participants)) +
  geom_col() +
  coord_flip() +
  labs(
    x = "Fictional universe",
    y = "Number of participants",
    title = "Distribution of universe selection across participants"
  ) +
  theme_minimal()

```

Participants have a large list of universes to pick from. The actual name of shows matters little here. The distribution of universes across participants is highly skewed. Most universes are selected by a relatively small number of participants. There is a high concentration of participants around a handful of very popular shows.

To operationnalize popularity as a category, we will introduce a median split separating higher than median selection as "popular" and lower than median as "unpopular".

Lets introduce a median split:

```{r}
#Introducing a median split of number of selection across universes, which we'll call median_selection.
median_selection <- median(universe_counts$n_participants)
median_selection
```

Our analysis categorizes universes by popularity, which here we compute by *frequency of selection*. We calculate the median of selection , which amounts to `{r} median_selection`\`. The median tells us that half of the available fictional universes were selected by fewer than 49 participants, and the other half is selected by more than 49 participants. This is the criteria we set to differentiate between popular and unpopular universes.

This median split allows us to both include all universes in our analysis and compare the mean ratings among two groups of different popularity levels. Lets classify the universes based on this criteria:

```{r}
#Building distinct groups by introducing the median split as a criteria. 
universe_counts <- universe_counts %>%
  mutate(
    popularity_group = if_else( #condition of median
      n_participants > median_selection,
      "Popular",
      "Unpopular" 
    )
  ) 
```

##### How do ratings differ by popularity ?

Now we want to see how each group performs on rating. Meaning, how similar do participants feel to the character they were matched with from a popular versus unpopular universe.

Reminder: The variable user rating is a Likert scale from 1 to 6, 1 being extremely similar and 6 extremely different. So here, the higher the rating, the less satisfied participants are with their matching.

```{r}
#dataframe that associates each universe with its rating 
ratings_by_universe <- data_sample_tidy %>% #extracting from original dataset
  select(user_id, chr_id, user_rating) %>%
  separate(chr_id, into = c("universe", "character_id"), sep = "/", remove = FALSE) %>%
  mutate(user_rating = as.numeric(user_rating)) #this allows us to work with means

#Now we show ratings by popularity of universes (bringing the criteria in the dataframe): 
ratings_with_popularity <- ratings_by_universe %>% 
  left_join(
    universe_counts %>% 
      select(universes_selected, popularity_group), #each universe is given its criteria depending on its rating
    by = c("universe" = "universes_selected")
  )

#Now calculating the mean, median and standard deviation of ratings by popularity
ratings_with_popularity %>%
  group_by(popularity_group) %>%
  summarise(
    n_ratings    = n(),
    mean_rating = mean(user_rating, na.rm = TRUE),
    median_rating = median(user_rating, na.rm = TRUE),
    sd_rating   = sd(user_rating, na.rm = TRUE)
  ) %>%
  mutate(
    mean_rating = round(mean_rating, 2),
    sd_rating   = round(sd_rating, 2)
  ) %>%
  kable(
    col.names = c(
      "Universe popularity",
      "Number of ratings",
      "Mean rating",
      "Median rating",
      "Standard deviation"
    ),
    caption = "Descriptive statistics of user ratings by universe popularity"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )
```

The mean ratings of popular versus unpopular shows is almost identical, with a difference of 0.06. Surprisingly, unpopular universes benefit from a silghtly lower rating on average, contradicting our initial prediction. Popular and unpopular universe groups seem to be associated with, on average, highly similar ratings. Lets visualize this with a box plot.

```{r}
#visualization of ratings by popularity: 

ggplot(
  ratings_with_popularity,
  aes(x = popularity_group, y = user_rating)
) +
  geom_boxplot() +
  labs(
    title = "Distribution of character ratings by universe popularity",
    x = "Universe popularity (median split)",
    y = "User rating (1 = extremely similar, 6 = extremely different)"
  ) +
  theme_minimal()

```

Indeed, as shown in this plot, popularity does not seem to be significantly associated with a change in rating. Although the range of ratings is higher in popular groups (which is expected), the means and standard deviations of the popularity categories are identical. Let's run a t-test to see if this is statistically sound.

##### Statistical test for this association (or lack thereof)

```{r}

t_test_res <- t.test(
  user_rating ~ popularity_group,
  data = ratings_with_popularity
)

tidy(t_test_res) %>%
  mutate(
    estimate_diff = estimate1 - estimate2
  ) %>%
  select(
    statistic, p.value, conf.low, conf.high,
    estimate1, estimate2, estimate_diff
  ) %>%
  kable(
    digits = 3,
    col.names = c(
      "t statistic", "p-value",
      "CI (low)", "CI (high)",
      "Mean (Popular)", "Mean (Unpopular)",
      "Mean difference"
    ),
    caption = "Welch two-sample t-test comparing user ratings by popularity group"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )



```

We performed a simple t-test given we are comparing two categorical variables. We use a t-test here to determine if there is a significant difference between the average (mean) ratings of popular and unpopular universes. Specifically, this test will show us if this difference is significant or due to random variation. Popularity_group is a binary variable with two categories: popular and unpopular. And rating is an ordinal categorical variable with six categories ranging from 1= Extremely similar to 6= extremely different.

A Welch two-sample t-test indicates that mean user ratings differ significantly between popular (M = 3.95) and unpopular items (M = 3.89), t(17,292) = 4.86, p \< .001, 95% CI \[0.04, 0.09\]. Although this test points to high statistical difference, the effect size is quite small (0.064). The statistical significance of this difference is most likely due to our large sample size.

Moreover, this difference in means goes in the opposite direction of our prediction. We hypothesized that popular universes would have *lower* ratings on average compared to unpopular universes. It seems that popularity has little effect on participants' ratings, and this effect comes from less known shows.

### Hypothesis 2: Unpopular shows' ratings are positively affected by year

Having established the negative relationship between popularity and similarity, we haven't explored the potential temporal aspect of this survey's results.

Particularly, unpopular shows may, overtime, garner more appreciation which could lead to even better similarity ratings. Highly popular shows like Harry potter or Game of thrones are quite old and might be surpassed by recently airing shows that are more salient culturally. New airing shows inhabit cultural discourse more intensely on the short term.

We hypothesize that, given that unpopular shows are already preferred rating wise, ratings will only get better as time goes compared to popular shows.

Translated to our variables' encryption, this means that ratings *for* *unpopular shows* will be lower as the variable year goes up.

##### How many participants took the survey throughout the years:

Let's visualize the distribution of participants by year

```{r}
#Renaming the variable year as survey_year: 
data_sample_tidy <- data_sample_tidy %>%
  rename (survey_year = year) #renaming variable as survey_year


#Barchart to visualise distribution of participants across years. 

data_sample_tidy %>%
  count(survey_year) %>%
  ggplot(aes(x = factor(survey_year), y = n)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Number of participants by survey year",
    x = "Survey year",
    y = "Number of participants"
  ) +
  theme_minimal()


```

Survey taking years range from 2020 to 2022, meaning three years. The number of participants decreased as time went by, by 50 000. 2020 had almost 80 000 participants, compared to 50 000 in 2021 and 30 000 in 2022.

##### Progression of ratings for all universes throughout these three years

```{r}
#First computing the average user_rating by year: 
avg_rating_year <- data_sample_tidy %>%
  group_by(survey_year) %>% #grouping each year by its average rating
  summarise(
    mean_rating = mean(user_rating, na.rm = TRUE),
    n = n()
  )
#Now plotting this progression: 
ggplot(avg_rating_year, aes(x = factor(survey_year), y = mean_rating)) +
  geom_point(size = 4) + #for clarity, we opt for points 
  geom_text(
    aes(label = round(mean_rating, 3)),
    vjust = -1,
    size = 4
  ) +
  labs(
    title = "Average similarity rating by survey year",
    x = "Survey year",
    y = "Average similarity rating"
  ) +
  ylim(3.85, 4.00) +
  theme_minimal()

```

This plot shows the average rating all universes got over the years, independent of their popularity category.

All average ratings for the three years of survey taking are around 4 (4="slightly different"). The average ratings from 2020 to 2021 went down 0.04, and then went back up again by 0.04 in 2022. This indicates that overall, participants in 2021 were slightly more satisfied with their character matching compared to the previous and next year. Lets now see this progression factoring in popularity of universes.

##### Change in rating over the years, by popularity:

```{r}
#new dataframe, showing each participant and year of taking survey
user_year <- data_sample_tidy %>%
  select(user_id, survey_year) %>%
  distinct(user_id, .keep_all = TRUE)

#Now merging dataframe created earlier showing universes and rating and their category  
ratings_with_popularity <- ratings_with_popularity %>%
  left_join(user_year, by = "user_id")


#Now we can see for each year, we see mean rating of all popular vs unpopular universes 
avg_rating_year_pop <- ratings_with_popularity %>%
  group_by(survey_year, popularity_group) %>%
  summarise(
    mean_rating = mean(user_rating, na.rm = TRUE),
    n_participant = n(),
    .groups = "drop"
  )
#putting this info in a table 
avg_rating_year_pop %>%
  mutate(
    mean_rating = round(mean_rating, 2)
  ) %>%
  kable(
    col.names = c(
      "Survey year",
      "Universe popularity",
      "Mean rating",
      "Number of participants"
    ),
    caption = "Mean similarity ratings by universe popularity and survey year"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )



```

The ratings across both years and popularity fluctuate moderately. The lowest rating is also the most recent, M=3.80 ; Y= 2022 for unpopular universes. The highest is recorded in the same year, 2022 at 3.97 for popular universes. Plotting these observations will help better visualize universes' rating progression throughout the years.

```{r}
#Visualising these fluctuations 
ggplot(
  avg_rating_year_pop,
  aes(
    x = factor(survey_year),#years on the x axis
    y = mean_rating,#average ratings on the y axis
    color = popularity_group,#two plotted lines per popularity grp, differing by color
    group = popularity_group
  )
) +
  geom_point(size = 3) +
  geom_line() +
  geom_text(
    aes(label = round(mean_rating, 3)),
    vjust = -1,
    size = 3,
    show.legend = FALSE
  ) +
  labs(
    title = "Average similarity rating by survey year, by universe popularity",
    x = "Survey year",
    y = "Average similarity rating",
    color = "Universe popularity"
  ) +
  theme_minimal()
```

This plot shows a clear divergence in ratings by popularity. Through the years, popular shows' ratings do not fluctuate significantly compared to unpopular shows. From 2020 to 2022, popular shows increase in rating, meaning that participants feel less similar to characters of these shows overtime. Conversely, unpopular shows show a consistent decrease in ratings by 0.15 over the three years of survey taking.

This pattern suggests that less popular universes may gain identificatory potential over time as they become more familiar to participants.

To understand more thoroughly the potential relationship between these three variables (ratings, year and popularity), and to also make sure this increase in similarity perception in unpopular universes is not due to less participants taking the survey through 2021 and 2022, we will perform an ANOVA.

#### ANOVA: Verifying if there is an interaction between popularity and year

We want to know if ratings are the product of an interaction between popularity and year.

Because user ratings are continuous and our predictors include both a categorical factor (popularity group) and a continuous variable (survey year), we first conduct a two-way ANOVA to assess whether mean ratings differ by popularity group and whether ratings *over time* vary across popularity groups. This allows us to test for statistically significant effects before looking at their direction (whether positive or negative).

ANOVA is a special case of the linear model in which predictors are categorical; therefore, ANOVA and linear regression rely on the same underlying assumptions:

**Normality of residuals:** The errors of our model should follow a normal distribution

**Homoscedasticity:** Residual variance is constant across all levels of fitted values.

```{r}
#First we will need to center the year variable so the model doesnt give us counterintuitive intercept and estimates.
ratings_with_popularity <- ratings_with_popularity %>%
  mutate(survey_year_c = survey_year - 2020)

#Second, we fit a linear model which assumes an interaction between year and popularity:
model_year <- lm(
  user_rating ~ popularity_group * survey_year_c,
  data = ratings_with_popularity
)
#Conduct a two way ANOVA to assess the significance of this interaction, plotted in a table for clarity: 
anova_tbl <- car::Anova(model_year, type = 2) %>% #this computs ANOVA results
  broom::tidy() %>% # this is to make a clean table
  mutate(
    p.value = format.pval(p.value, digits = 3, eps = .001),
    statistic = round(statistic, 2) #this rounds very small values to zero
  ) %>%
  select(term, df, statistic, p.value) %>% #the values needed to interpret
  rename(`Effect` = term, `Df` = df, `F` = statistic, `p` = p.value)

anova_tbl %>% #here fitted into a pretty table
  kbl(caption = "Type II ANOVA for user_rating") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))

```

This two-way ANOVA test gives us insight on the effect of popularity on ratings, the effect of years on ratings, and the interaction between popularity and years.

**Popularity_group:** There is a significant main effect of popularity, with a high F-ratio (F = 23.12), and low p-value (p \< .001). Meaning that the difference in means between popular and unpopular groups is indeed significant.

**Survey_year:** Survey year has no significant effect on ratings, F= 1.58, p = .21. F-ration is low and high p-value lead to accepting the null hypothesis that inter and intra-group variation are equal.

**Popularity_group:survey_year_c:** The interaction between popularity and survey year is statistically significant, F = 18.09, p \< .001, indicating that rating trends over time differ between popular and unpopular shows.

Checking for the model's assumptions let's see the distribution of residuals:

```{r}
plot(model_year, which = 2)
```

Residuals following perfect randomization would align perfectly with the dotted diagonal line. Given that our outcome variable, ratings, can only take certain values (from one to six), it's expected we see a stair-case like shape as shown in the plot. Given our large sample size, although the residuals arent perfectly normal, we have sufficient normality for statistical inference (meaning our F-test and p-values) to remain valid. Again, our interpretations require caution.

Checking for homoscedasticity:

```{r}
plot(model_year, which = 1)
```

If the outcome variable were perfectly continuous, we would expect the residuals to form a random cloud around the zero line. However, as discussed earlier, the rating variable can only take a limited set of discrete values, which explains the vertically stacked pattern observed in the plot. Importantly, the spread of residuals remains relatively constant across fitted values, indicating that the homoscedasticity assumption is reasonably satisfied.

Having confirmed the *existence* of an effect, particularly an interaction between years and universe popularity, its time we build a regression table to know more about the direction of this interaction and its effect size.

#### Regression table and regression line. 

```{r}
#We build a regression table showing first a linear model predicting ratings by popularity, a relationship we've analysed before in Hypothesis 1:
m1 <- lm(
  user_rating ~ popularity_group,
  data = ratings_with_popularity
)
#Our second model introduces interaction with the variable survey_year: 
m2 <- lm(
  user_rating ~ popularity_group * survey_year_c,
  data = ratings_with_popularity
)
#we list these two models in order to polt them in a regression table
models <- list(
  "Popularity only" = m1,
  "Popularity √ó Year" = m2
)

#the modelsummary() function allows us to see all important statistical inference 
modelsummary(
  models,
  output = "kableExtra",
  statistic = "({std.error})",
  stars = TRUE,
  gof_map = c("nobs", "r.squared", "adj.r.squared", "aic", "bic"),
  title = "Linear models predicting user ratings"
) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))

```

**Model 1:**

X = ùõΩ0 + ùõΩ1. Y

In M1, the outcome variable is ratings (X) and the predictor is popularity group (Y).

This model assumes a linear relationship between user ratings and popularity group. The intercept, which represents the average rating for shows in the reference category (here, popular), is estimated at 3.952 (p \< 0.001). The coefficient associated with the unpopular category is ‚àí0.064 (p \< 0.001), indicating that, on average, unpopular shows receive ratings that are 0.064 points lower than popular shows. This difference is statistically significant. However, the R¬≤ value (‚âà 0) indicates that popularity alone explains a negligible share of the overall variation in user ratings, suggesting that while the effect is statistically detectable, it is substantively very small.

**Model 2:**

X=ùõΩ0 + ùõΩ1. Y + ùõΩ2. Z + ùõΩ3 (Y\*Z)

In M2, X is still ratings, but we introduce a new predictor (Z), survey_year, and the interaction between survey_year and popularity_group (Y\*Z).

This model extends the previous specification by allowing the effect of popularity on ratings to vary over time. The intercept represents the expected rating for popular shows in the reference year (2020) and is estimated at 3.953 (p \< 0.001). The main effect of popularity group is no longer statistically significant, indicating that, in the reference year, average ratings for popular and unpopular shows do not meaningfully differ. The main effect of survey year is also close to zero and not statistically significant for popular shows. However, the interaction term between popularity group and survey year is negative and statistically significant (‚àí0.078, p \< 0.001), indicating that ratings for unpopular shows decline over time relative to popular shows. As in Model 1, the R¬≤ value remains close to zero, suggesting that although the interaction effect is statistically significant, the model explains very little of the overall variation in user ratings.

The model allowed to discover that the direction of the effect of interaction between years and popularity follows our hypothesis. Unpopular show benefit overtime from more positive ratings. However, given the r¬≤ of M2, no actual variation is explained by this interaction.

```{r}
#Here we visualize our model in a dotted plot: 
ggplot(ratings_with_popularity,
       aes(x = survey_year_c,
           y = user_rating,
           color = popularity_group)) +
  geom_point(alpha = 0.15) +
  geom_smooth(method = "lm", se = FALSE) +#this introduces a smoth line showing model predictions for both popular and unpopular groups
  labs(
    x = "Survey year (centered)",
    y = "User rating",
    color = "Popularity group",
    title = "User ratings over time by popularity group"
  ) +
  theme_minimal()

```

This dotted plot reinforces our previous interpretation, the model captures 0 variation. There is a negative slope for unpopular shows, but the flat fitted values don't at all follow model predictions.

## Conclusion

Overall, our hypotheses are not supported. Even though some effects are statistically significant, the models explain almost none of the variation in user ratings, suggesting that popularity and its interaction with time do not meaningfully shape how characters are rated. This may partly be due to data limitations: although the survey is still ongoing, we only observe ratings over three years, which makes it difficult to clearly identify any interaction between popularity and time. With more data, these dynamics might become more visible. In addition, including information on participants‚Äô age could be an interesting extension, as people from different age groups may not only select different universes but also evaluate characters from older and newer shows in different ways.
